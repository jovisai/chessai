[
  {
    "objectID": "hello.html",
    "href": "hello.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "chess_board_segmentation.html",
    "href": "chess_board_segmentation.html",
    "title": "Chess Board Segmentation",
    "section": "",
    "text": "Applying Color Filtering to Image to isolate Red color\nTo segment the chess board from the environment, I am using a trick by coloring the boundaries of my chessboard as you can see in the below image.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport cv2\n\nimage = cv2.imread('data/chess_algo_1.jpg')\nimage_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# convert to LAB color space\nlab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n\n# Perform Otsu threshold on the A-channel \nth = cv2.threshold(lab[:,:,1], 127, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n\nresult = cv2.bitwise_and(image, image, mask=th)\n\n# Display the image using Matplotlib\nfig, ax = plt.subplots(nrows=1, ncols=2)\nax[0].set_title('Loaded Sample Image')\nax[0].imshow(image_rgb)\nax[0].axis('off')\nax[1].set_title('Red Color Segmented')\nax[1].imshow(result)\nax[1].axis('off')\n\nplt.tight_layout()\nplt.axis('off')\n# Display the plot\nplt.show()\n\n\n\n\n\nThe first step in the segmentation algorithm is to use the red boundary and isolate it from the full image.\nThis code snippet demonstrates how to perform color-based segmentation using the LAB color space and Otsu thresholding. Here’s a breakdown of the code:\n\nConvert to LAB Color Space:\n\nlab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB): Converts the original image from the default BGR color space to the LAB color space. The LAB color space consists of three channels: L (Lightness), A (green-magenta component), and B (blue-yellow component). This conversion is performed using the cvtColor function from OpenCV.\n\nPerform Otsu Thresholding on the A-channel:\n\nth = cv2.threshold(lab[:,:,1], 127, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]: Extracts the A-channel (green-magenta component) from the LAB image and applies Otsu’s thresholding technique to segment the image into foreground and background regions. Otsu’s thresholding automatically calculates the optimal threshold value based on the image histogram. The resulting binary threshold image is stored in the variable th.\n\nApply the Mask:\n\nimage = cv2.bitwise_and(image, image, mask=th): Applies the binary threshold mask to the original image using the bitwise_and function from OpenCV. This operation retains only the pixels in the original image that correspond to the foreground regions identified by the threshold mask. The mask argument specifies the binary mask to be applied.\n\n\nAfter executing this code, the image variable will hold the processed image, where only the foreground regions, determined by Otsu’s thresholding on the A-channel, are visible, and the background is set to black.\n\n\nIdentifying all the lines in the image using Classic straight-line Hough transform\nThe Hough transform is a simple algorithm commonly used in computer vision to detect lines and shapes in an image. It provides a robust method to identify geometric patterns by representing them in a parameter space known as the Hough space. The algorithm works by converting image space coordinates to parameter space, where each point in the parameter space corresponds to a possible line or shape in the image. By accumulating votes for different parameter combinations, the Hough transform identifies the most prominent lines or shapes based on the peaks in the parameter space. This approach is particularly useful for line detection, as it can handle various types of lines, including straight lines, curves, and even partially occluded or broken lines.\n\n\nCode\nfrom skimage.transform import hough_line, hough_line_peaks\nfrom skimage.color import rgb2gray\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimage_gray = rgb2gray(image_rgb)\n\n# Classic straight-line Hough transform\n# Set a precision of 0.5 degree.\ntested_angles = np.linspace(-np.pi / 2, np.pi / 2, 360, endpoint=False)\nh, theta, d = hough_line(rgb2gray(result), theta=tested_angles)\n\nplt.figure(figsize=(20,10))\nplt.imshow(image_gray, cmap='gray')\n\nfor _, angle, dist in zip(*hough_line_peaks(h, theta, d)):\n    (x0, y0) = dist * np.array([np.cos(angle), np.sin(angle)])\n    plt.axline((x0, y0), slope=np.tan(angle + np.pi/2))\n\nplt.title('Detected lines')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nHere is the explanation of the key part of code which does line detection using the Hough transform:\n\nGenerating tested angles\n\ntested_angles = np.linspace(-np.pi / 2, np.pi / 2, 360, endpoint=False)\nIn this line, the np.linspace() function generates an array of 360 equally spaced angles between -np.pi / 2 and np.pi / 2. These angles represent the range of lines to be tested during the Hough transform. The endpoint=False argument ensures that the endpoint is not included in the generated array.\n\nPerforming the Hough transform\n\nh, theta, d = hough_line(rgb2gray(result), theta=tested_angles)\nHere, the hough_line() function is applied to the grayscale version of the result image using rgb2gray() to convert it. The theta parameter is set to the tested_angles array, which specifies the angles to consider during the transform. The resulting Hough accumulator array h, along with the theta angles theta and distances d, are stored.\n\n\nCode\nplt.title(\"hough transform visualization\")\nplt.imshow(np.log(1 + h),\n           extent=[np.rad2deg(theta[-1]), np.rad2deg(theta[0]), d[-1], d[0]],\n           cmap='gray', aspect='auto')\n\n\n&lt;matplotlib.image.AxesImage at 0x7f564ed0edd0&gt;\n\n\n\n\n\n\nDetecting and visualizing the lines\n\nfor _, angle, dist in zip(*hough_line_peaks(h, theta, d)):\n    (x0, y0) = dist * np.array([np.cos(angle), np.sin(angle)])\n    plt.axline((x0, y0), slope=np.tan(angle + np.pi/2))\nThis loop iterates through the peaks detected in the Hough transform using the hough_line_peaks() function. For each peak, the angle and dist values represent the orientation and distance from the origin to a line in the image. The (x0, y0) coordinates are calculated by multiplying the distance with the [np.cos(angle), np.sin(angle)] vector, which determines the line’s position. Finally, plt.axline() is used to draw a line on the plot using the calculated (x0, y0) coordinates and the tangent of the angle plus np.pi/2.\nBy running this code, you will perform the Hough transform to detect lines in an image. The resulting lines will be visualized on a plot using plt.axline(). This code segment is useful for line detection applications and helps to understand the concept of identifying lines in an image using the Hough transform.\n\n\nFind all points of intersections of the lines\nTo extract the end points of the chess board, we need to find the intersection of the Hough lines.\n\n\nCode\nimport math \n\ndef find_intersection_point(fp_x0, fp_y0, slope1, sp_x0, sp_y0, slope2):\n    if (slope1 - slope2) == 0:\n      return []\n\n    # Calculate the intersection point coordinates\n    x_intersect = (sp_y0 - fp_y0 + slope1 * fp_x0 - slope2 * sp_x0) / (slope1 - slope2)\n    y_intersect = slope1 * (x_intersect - fp_x0) + fp_y0\n\n    if x_intersect &lt; 0 or y_intersect &lt; 0 or x_intersect &gt; 4000 or y_intersect &gt; 4000:\n      return []\n\n    angle_of_intersection =  math.degrees(math.atan((slope1-slope2)/(1+slope1*slope2)))\n\n    if angle_of_intersection &lt; 45 and angle_of_intersection &gt; -45:\n      return []\n\n    # Intersection point coordinates\n    intersection_point = [x_intersect, y_intersect]\n\n    return intersection_point\n\nlines = []\nall_points_and_slopes = []\nfor _, angle, dist in zip(*hough_line_peaks(h, theta, d)):\n    (x0, y0) = dist * np.array([np.cos(angle), np.sin(angle)])\n    lines.append([x0, y0, angle])\n    # ax[2].axline((x0, y0), slope=np.tan(angle + np.pi/2))\n    # ax[2].scatter(x0, y0)\n    slope = np.tan(angle + np.pi/2)\n    all_points_and_slopes.append([x0, y0, slope])\n\n# find intersection points\nintersection_points = []\nfor i in range(len(all_points_and_slopes)):\n  for j in range(1, len(all_points_and_slopes)):\n    p1 = all_points_and_slopes[i]\n    p2 = all_points_and_slopes[j]\n\n    ip = find_intersection_point(p1[0], p1[1], p1[2], p2[0], p2[1], p2[2])\n    if ip:\n      intersection_points.append(ip)\n\nintersection_points = np.array(intersection_points)\n\nplt.figure(figsize=(20,10))\nplt.imshow(image_gray, cmap='gray')\nplt.scatter(intersection_points[:, 0], intersection_points[:, 1], color='r')\nplt.title('Detected points of intersections')\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nBut as you can see, there are simply too many points due to too many intersecting lines. We are mainly intereseted in just 4 points which represent the 4 corners of the chess board. We can use KMeans algorithm with a cluster size of 4 to group the close together points. The K-means clustering algorithm is commonly used for unsupervised learning tasks to group similar data points together. It is an iterative algorithm that aims to minimize the within-cluster variance by adjusting the cluster centroids until convergence.\n\n\nCode\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=4, random_state=0, n_init=\"auto\").fit(intersection_points)\n\nplt.figure(figsize=(20,10))\nplt.imshow(image_gray, cmap='gray')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], color='r')\nplt.title('Detected points of intersections')\n\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nkmeans = KMeans(n_clusters=4, random_state=0, n_init=\"auto\").fit(intersection_points)\nHere, the KMeans class is instantiated with the following parameters: - n_clusters=4: Specifies the number of clusters to form. In this case, we want to create four clusters. - random_state=0: Sets the random seed for reproducibility. By setting a specific random state value, the clustering results will be the same each time the code is run with the same data. - n_init=\"auto\": Determines the number of times the K-means algorithm will be run with different centroid seeds. “auto” automatically selects a value based on the number of data points.\nThe fit() method is then called on the KMeans object, with intersection_points as the input data. This fits the K-means model to the data, performing the clustering and assigning each data point to one of the four clusters.\nThe result of running the fit() method is stored in the kmeans variable. This object contains information about the fitted K-means model, including the cluster assignments for each data point.\nBy examining the kmeans object, you can access various properties and methods, such as kmeans.labels_ to retrieve the assigned cluster labels for each data point or kmeans.cluster_centers_ to obtain the centroid coordinates of each cluster.\n\n\nConnect the 4 points into a polygon\nWe use the standard convexHull algorithm to sort the 4 points in the order in which one can connect them into a polygon. Convex hull is a concept in computational geometry that represents the smallest convex polygon that encloses a given set of points in a plane.\n\n\nCode\nfrom scipy.spatial import ConvexHull\npoints = kmeans.cluster_centers_\nhull = ConvexHull(points)\n\nplt.figure(figsize=(20,10))\nplt.imshow(image_gray, cmap='gray')\nplt.plot(points[:,0], points[:,1], 'o', color='r')\nfor simplex in hull.simplices:\n    plt.plot(points[simplex, 0], points[simplex, 1], color='r')\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nRemove the rest of the environment\nIts a simple crop based on the polygon I generated from Convexhull calculation\n\n\nCode\nfrom skimage import draw\n\npoints = kmeans.cluster_centers_\npolygon_points = []\nfor k in hull.vertices:\n  polygon_points.append(points[k])\n\npolygon_points = np.array(polygon_points)\n\nprint(polygon_points)\n\n# Create a mask of the polygon region\nmask = np.zeros(image_rgb.shape[:2], dtype=np.uint8)\nrr, cc = draw.polygon(polygon_points[:, 1], polygon_points[:, 0])\nmask[rr, cc] = 1\n\n# Apply the mask to the input image\ncropped_image = image_rgb.copy()\ncropped_image[mask == 0] = 0\nplt.figure(figsize=(20,10))\nplt.imshow(cropped_image)\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n\n[[ 928.50000117 1361.24373308]\n [ 147.88027356 1358.97685454]\n [ 261.00292634  848.75466668]\n [ 827.86563763  843.80773074]]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Muthukrishnan",
    "section": "",
    "text": "Hey there, fellow adventurers! Welcome to my corner of this research notebook. I’m Muthukrishnan, your friendly neighborhood software engineer.\nI thrive on curiosity and a thirst for knowledge, constantly seeking new experiences and learning opportunities. Whether it’s diving into the depths of a gripping novel or immersing myself in the vibrant world of coding, I find joy in expanding my horizons.\nAs a tech enthusiast, I’m constantly amazed by the boundless possibilities it brings. From tinkering with gadgets to exploring the ever-evolving world of AI, I’m enthralled by the exciting intersection of humanity and innovation.\nIf you ask me my favorite miracles, it would be the Euler’s Identity and the Butterfly curve.\nEuler’s Identity \\[\\begin{gather*}\ne^{i\\pi }+1=0\n\\end{gather*}\\]\nButterfly curve \\[\\begin{gather*}\n{\\displaystyle x=\\sin t\\!\\left(e^{\\cos t}-2\\cos 4t-\\sin ^{5}\\!{\\Big (}{t \\over 12}{\\Big )}\\right)}\n\\end{gather*}\\]\n\\[\\begin{gather*}\n{\\displaystyle y=\\cos t\\!\\left(e^{\\cos t}-2\\cos 4t-\\sin ^{5}\\!{\\Big (}{t \\over 12}{\\Big )}\\right)}\n\\end{gather*}\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameterize the butterfly curve equation\nt = np.linspace(0, 2 * np.pi, 1000)\nx = np.sin(t) * (np.exp(np.cos(t)) - 2 * np.cos(4 * t) - np.sin(t / 12) ** 5)\ny = np.cos(t) * (np.exp(np.cos(t)) - 2 * np.cos(4 * t) - np.sin(t / 12) ** 5)\n\n# Plot the butterfly curve\nplt.plot(x, y, color='red', linewidth=1)\nplt.title(\"Butterfly curve\")\nplt.axis('equal')\n\n# Display the plot\nplt.show()\n\n\n\n\n\nSo, dear reader, join me on this thrilling journey as we navigate through the twists and turns of life. Together, let’s discover new passions, celebrate the extraordinary, and revel in the joy of being wonderfully unique.\nFasten your seatbelts, embrace your sense of wonder, and let’s embark on an unforgettable adventure together!\nI also blog at muthu.co, you can read about the Euler’s formula here"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computer Vision Notebooks",
    "section": "",
    "text": "Muthukrishnan\n\n\nHey there, fellow adventurers! Welcome to my corner of this research notebook. I’m Muthukrishnan, your friendly neighborhood software engineer.\nI thrive on curiosity and a thirst for knowledge, constantly seeking new experiences and learning opportunities. Whether it’s diving into the depths of a gripping novel or immersing myself in the vibrant world of coding, I find joy in expanding my horizons.\nAs a tech enthusiast, I’m constantly amazed by the boundless possibilities it brings. From tinkering with gadgets to exploring the ever-evolving world of AI, I’m enthralled by the exciting intersection of humanity and innovation.\nIf you ask me my favorite miracles, it would be the Euler’s Identity and the Butterfly curve.\nEuler’s Identity \\[\\begin{gather*}\ne^{i\\pi }+1=0\n\\end{gather*}\\]\nButterfly curve \\[\\begin{gather*}\n{\\displaystyle x=\\sin t\\!\\left(e^{\\cos t}-2\\cos 4t-\\sin ^{5}\\!{\\Big (}{t \\over 12}{\\Big )}\\right)}\n\\end{gather*}\\]\n\\[\\begin{gather*}\n{\\displaystyle y=\\cos t\\!\\left(e^{\\cos t}-2\\cos 4t-\\sin ^{5}\\!{\\Big (}{t \\over 12}{\\Big )}\\right)}\n\\end{gather*}\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameterize the butterfly curve equation\nt = np.linspace(0, 2 * np.pi, 1000)\nx = np.sin(t) * (np.exp(np.cos(t)) - 2 * np.cos(4 * t) - np.sin(t / 12) ** 5)\ny = np.cos(t) * (np.exp(np.cos(t)) - 2 * np.cos(4 * t) - np.sin(t / 12) ** 5)\n\n# Plot the butterfly curve\nplt.plot(x, y, color='red', linewidth=1)\nplt.title(\"Butterfly curve\")\nplt.axis('equal')\n\n# Display the plot\nplt.show()\n\n\n\n\n\nSo, dear reader, join me on this thrilling journey as we navigate through the twists and turns of life. Together, let’s discover new passions, celebrate the extraordinary, and revel in the joy of being wonderfully unique.\nFasten your seatbelts, embrace your sense of wonder, and let’s embark on an unforgettable adventure together!\nI also blog at muthu.co, you can read about the Euler’s formula here"
  },
  {
    "objectID": "utilities/video to images.html",
    "href": "utilities/video to images.html",
    "title": "Extract Image frames from Video",
    "section": "",
    "text": "Code\nimport skimage.io\nimport os\n\n# Get the path to the video file.\nvideo_file = \"utilities/data/VOD_20230708_062617.mp4\"\n\nimport cv2\nimport os\n\n# Create a directory to store the images.\nimage_dir = \"video_frames\"\nif not os.path.exists(image_dir):\n    os.mkdir(image_dir)\n\n# Get the number of frames in the video.\ncap = cv2.VideoCapture(video_file)\nnum_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n# Iterate over the frames in the video and save them as images.\nfor i in range(num_frames):\n    ret, frame = cap.read()\n    if not ret:\n        continue\n    image_name = \"frame_{}.jpg\".format(i)\n    cv2.imwrite(os.path.join(image_dir, image_name), frame)\n\ncap.release()"
  },
  {
    "objectID": "utilities/video_to_images.html",
    "href": "utilities/video_to_images.html",
    "title": "Extract Image frames from Video",
    "section": "",
    "text": "import skimage.io\nimport os\n\n# Get the path to the video file.\nvideo_file = \"data/VOD_20230708_195627.mp4\"\n\nimport cv2\nimport os\n\n# Create a directory to store the images.\nimage_dir = \"video_frames\"\nif not os.path.exists(image_dir):\n    os.mkdir(image_dir)\n\n# Get the number of frames in the video.\ncap = cv2.VideoCapture(video_file)\nnum_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n# Iterate over the frames in the video and save them as images.\nfor i in range(num_frames):\n    ret, frame = cap.read()\n    if not ret:\n        continue\n    image_name = \"frame_{}.jpg\".format(i)\n    cv2.imwrite(os.path.join(image_dir, image_name), frame)\n\ncap.release()"
  },
  {
    "objectID": "chess_board_segmentation_with_flat_board.html",
    "href": "chess_board_segmentation_with_flat_board.html",
    "title": "Chess Board Segmentation with gray white board",
    "section": "",
    "text": "Applying Color Filtering to Image to isolate Red color\nTo segment the chess board from the environment, I am using a trick by coloring the boundaries of my chessboard as you can see in the below image.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport cv2\nimport urllib\nimport numpy as np\n\nimage = cv2.imread('data/frame_3.jpg')\n\nimage_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n# convert to LAB color space\nlab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n\n# Perform Otsu threshold on the A-channel \nth = cv2.threshold(lab[:,:,1], 127, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n\nresult = cv2.bitwise_and(image, image, mask=th)\n\n# Display the image using Matplotlib\nfig, ax = plt.subplots(nrows=1, ncols=2)\nax[0].set_title('Loaded Sample Image')\nax[0].imshow(image_rgb)\nax[0].axis('off')\nax[1].set_title('Red Color Segmented')\nax[1].imshow(result)\nax[1].axis('off')\n\nplt.tight_layout()\nplt.axis('off')\n# Display the plot\nplt.show()\n\n\n\n\n\nThe first step in the segmentation algorithm is to use the red boundary and isolate it from the full image.\nThis code snippet demonstrates how to perform color-based segmentation using the LAB color space and Otsu thresholding. Here’s a breakdown of the code:\n\nConvert to LAB Color Space:\n\nlab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB): Converts the original image from the default BGR color space to the LAB color space. The LAB color space consists of three channels: L (Lightness), A (green-magenta component), and B (blue-yellow component). This conversion is performed using the cvtColor function from OpenCV.\n\nPerform Otsu Thresholding on the A-channel:\n\nth = cv2.threshold(lab[:,:,1], 127, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]: Extracts the A-channel (green-magenta component) from the LAB image and applies Otsu’s thresholding technique to segment the image into foreground and background regions. Otsu’s thresholding automatically calculates the optimal threshold value based on the image histogram. The resulting binary threshold image is stored in the variable th.\n\nApply the Mask:\n\nimage = cv2.bitwise_and(image, image, mask=th): Applies the binary threshold mask to the original image using the bitwise_and function from OpenCV. This operation retains only the pixels in the original image that correspond to the foreground regions identified by the threshold mask. The mask argument specifies the binary mask to be applied.\n\n\nAfter executing this code, the image variable will hold the processed image, where only the foreground regions, determined by Otsu’s thresholding on the A-channel, are visible, and the background is set to black.\n\n\nIdentifying all the lines in the image using Classic straight-line Hough transform\nThe Hough transform is a simple algorithm commonly used in computer vision to detect lines and shapes in an image. It provides a robust method to identify geometric patterns by representing them in a parameter space known as the Hough space. The algorithm works by converting image space coordinates to parameter space, where each point in the parameter space corresponds to a possible line or shape in the image. By accumulating votes for different parameter combinations, the Hough transform identifies the most prominent lines or shapes based on the peaks in the parameter space. This approach is particularly useful for line detection, as it can handle various types of lines, including straight lines, curves, and even partially occluded or broken lines.\n\n\nCode\nfrom skimage.transform import hough_line, hough_line_peaks\nfrom skimage.color import rgb2gray\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimage_gray = rgb2gray(image_rgb)\n\n# Classic straight-line Hough transform\n# Set a precision of 0.5 degree.\ntested_angles = np.linspace(-np.pi / 2, np.pi / 2, 360, endpoint=False)\nh, theta, d = hough_line(rgb2gray(result), theta=tested_angles)\n\nplt.figure(figsize=(20,10))\nplt.imshow(image_gray, cmap='gray')\n\nfor _, angle, dist in zip(*hough_line_peaks(h, theta, d)):\n    (x0, y0) = dist * np.array([np.cos(angle), np.sin(angle)])\n    plt.axline((x0, y0), slope=np.tan(angle + np.pi/2))\n\nplt.title('Detected lines')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nHere is the explanation of the key part of code which does line detection using the Hough transform:\n\nGenerating tested angles\n\ntested_angles = np.linspace(-np.pi / 2, np.pi / 2, 360, endpoint=False)\nIn this line, the np.linspace() function generates an array of 360 equally spaced angles between -np.pi / 2 and np.pi / 2. These angles represent the range of lines to be tested during the Hough transform. The endpoint=False argument ensures that the endpoint is not included in the generated array.\n\nPerforming the Hough transform\n\nh, theta, d = hough_line(rgb2gray(result), theta=tested_angles)\nHere, the hough_line() function is applied to the grayscale version of the result image using rgb2gray() to convert it. The theta parameter is set to the tested_angles array, which specifies the angles to consider during the transform. The resulting Hough accumulator array h, along with the theta angles theta and distances d, are stored.\n\n\nCode\nplt.title(\"hough transform visualization\")\nplt.imshow(np.log(1 + h),\n           extent=[np.rad2deg(theta[-1]), np.rad2deg(theta[0]), d[-1], d[0]],\n           cmap='gray', aspect='auto')\n\n\n&lt;matplotlib.image.AxesImage at 0x7f735e2ba530&gt;\n\n\n\n\n\n\nDetecting and visualizing the lines\n\nfor _, angle, dist in zip(*hough_line_peaks(h, theta, d)):\n    (x0, y0) = dist * np.array([np.cos(angle), np.sin(angle)])\n    plt.axline((x0, y0), slope=np.tan(angle + np.pi/2))\nThis loop iterates through the peaks detected in the Hough transform using the hough_line_peaks() function. For each peak, the angle and dist values represent the orientation and distance from the origin to a line in the image. The (x0, y0) coordinates are calculated by multiplying the distance with the [np.cos(angle), np.sin(angle)] vector, which determines the line’s position. Finally, plt.axline() is used to draw a line on the plot using the calculated (x0, y0) coordinates and the tangent of the angle plus np.pi/2.\nBy running this code, you will perform the Hough transform to detect lines in an image. The resulting lines will be visualized on a plot using plt.axline(). This code segment is useful for line detection applications and helps to understand the concept of identifying lines in an image using the Hough transform.\n\n\nFind all points of intersections of the lines\nTo extract the end points of the chess board, we need to find the intersection of the Hough lines.\n\n\nCode\nimport math \n\ndef find_intersection_point(fp_x0, fp_y0, slope1, sp_x0, sp_y0, slope2):\n    if (slope1 - slope2) == 0:\n      return []\n\n    # Calculate the intersection point coordinates\n    x_intersect = (sp_y0 - fp_y0 + slope1 * fp_x0 - slope2 * sp_x0) / (slope1 - slope2)\n    y_intersect = slope1 * (x_intersect - fp_x0) + fp_y0\n\n    if x_intersect &lt; 0 or y_intersect &lt; 0 or x_intersect &gt; 4000 or y_intersect &gt; 4000:\n      return []\n\n    angle_of_intersection =  math.degrees(math.atan((slope1-slope2)/(1+slope1*slope2)))\n\n    if angle_of_intersection &lt; 45 and angle_of_intersection &gt; -45:\n      return []\n\n    # Intersection point coordinates\n    intersection_point = [x_intersect, y_intersect]\n\n    return intersection_point\n\nlines = []\nall_points_and_slopes = []\nfor _, angle, dist in zip(*hough_line_peaks(h, theta, d)):\n    (x0, y0) = dist * np.array([np.cos(angle), np.sin(angle)])\n    lines.append([x0, y0, angle])\n    # ax[2].axline((x0, y0), slope=np.tan(angle + np.pi/2))\n    # ax[2].scatter(x0, y0)\n    slope = np.tan(angle + np.pi/2)\n    all_points_and_slopes.append([x0, y0, slope])\n\n# find intersection points\nintersection_points = []\nfor i in range(len(all_points_and_slopes)):\n  for j in range(1, len(all_points_and_slopes)):\n    p1 = all_points_and_slopes[i]\n    p2 = all_points_and_slopes[j]\n\n    ip = find_intersection_point(p1[0], p1[1], p1[2], p2[0], p2[1], p2[2])\n    if ip:\n      intersection_points.append(ip)\n\nintersection_points = np.array(intersection_points)\n\nplt.figure(figsize=(20,10))\nplt.imshow(image_gray, cmap='gray')\nplt.scatter(intersection_points[:, 0], intersection_points[:, 1], color='r')\nplt.title('Detected points of intersections')\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nBut as you can see, there are simply too many points due to too many intersecting lines. We are mainly intereseted in just 4 points which represent the 4 corners of the chess board. We can use KMeans algorithm with a cluster size of 4 to group the close together points. The K-means clustering algorithm is commonly used for unsupervised learning tasks to group similar data points together. It is an iterative algorithm that aims to minimize the within-cluster variance by adjusting the cluster centroids until convergence.\n\n\nCode\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=4, random_state=0, n_init=\"auto\").fit(intersection_points)\n\nplt.figure(figsize=(20,10))\nplt.imshow(image_gray, cmap='gray')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], color='r')\nplt.title('Detected points of intersections')\n\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nkmeans = KMeans(n_clusters=4, random_state=0, n_init=\"auto\").fit(intersection_points)\nHere, the KMeans class is instantiated with the following parameters: - n_clusters=4: Specifies the number of clusters to form. In this case, we want to create four clusters. - random_state=0: Sets the random seed for reproducibility. By setting a specific random state value, the clustering results will be the same each time the code is run with the same data. - n_init=\"auto\": Determines the number of times the K-means algorithm will be run with different centroid seeds. “auto” automatically selects a value based on the number of data points.\nThe fit() method is then called on the KMeans object, with intersection_points as the input data. This fits the K-means model to the data, performing the clustering and assigning each data point to one of the four clusters.\nThe result of running the fit() method is stored in the kmeans variable. This object contains information about the fitted K-means model, including the cluster assignments for each data point.\nBy examining the kmeans object, you can access various properties and methods, such as kmeans.labels_ to retrieve the assigned cluster labels for each data point or kmeans.cluster_centers_ to obtain the centroid coordinates of each cluster.\n\n\nConnect the 4 points into a polygon\nWe use the standard convexHull algorithm to sort the 4 points in the order in which one can connect them into a polygon. Convex hull is a concept in computational geometry that represents the smallest convex polygon that encloses a given set of points in a plane.\n\n\nCode\nfrom scipy.spatial import ConvexHull\npoints = kmeans.cluster_centers_\nhull = ConvexHull(points)\n\nplt.figure(figsize=(20,10))\nplt.imshow(image_gray, cmap='gray')\nplt.plot(points[:,0], points[:,1], 'o', color='r')\nfor simplex in hull.simplices:\n    plt.plot(points[simplex, 0], points[simplex, 1], color='r')\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nRemove the rest of the environment\nIts a simple crop based on the polygon I generated from Convexhull calculation\n\n\nCode\nfrom skimage import draw\n\npoints = kmeans.cluster_centers_\npolygon_points = []\nfor k in hull.vertices:\n  polygon_points.append(points[k])\n\npolygon_points = np.array(polygon_points)\n\nprint(polygon_points)\n\n# Create a mask of the polygon region\nmask = np.zeros(image_rgb.shape[:2], dtype=np.uint8)\nrr, cc = draw.polygon(polygon_points[:, 1], polygon_points[:, 0])\nmask[rr, cc] = 1\n\n# Apply the mask to the input image\ncropped_image = image_rgb.copy()\ncropped_image[mask == 0] = 0\nplt.figure(figsize=(20,10))\nplt.imshow(cropped_image)\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n\n[[1563.08639326 3231.0456892 ]\n [ 160.00356693 2540.21979825]\n [ 903.63144527 1588.4195179 ]\n [2058.46128992 2020.18400548]]\n\n\n\n\n\n\n\nProjective transform the board alone\nProjective transform, also known as perspective transform or homography, is a fundamental concept in image processing and computer vision. It is a geometric transformation used to map points from one plane (2D space) to another plane (also 2D space) in a way that preserves straight lines. This transformation is particularly useful when dealing with images captured from different viewpoints or with varying camera angles.\nIn simpler terms, projective transform allows us to correct the perspective distortion in an image by transforming it into a new view as if it was captured from a different angle or position.\nThe projective transform is represented by a 3x3 matrix known as the homography matrix. This matrix is determined by a set of corresponding points between two images or scenes. Typically, at least four pairs of corresponding points are needed to calculate the homography matrix.\nLet’s assume we have two sets of corresponding points:\nSource points (X, Y) = {(x1, y1), (x2, y2), (x3, y3), (x4, y4)}\nDestination points (X', Y') = {(x1', y1'), (x2', y2'), (x3', y3'), (x4', y4')}\nThe goal is to find a 3x3 matrix H such that, when we apply the projective transform to each point (x, y) in the source image, it will be mapped to a new point (x’, y’) in the destination image:\n| x' |   | h11 h12 h13 |   | x |\n| y' | = | h21 h22 h23 | * | y |\n| 1  |   | h31 h32 h33 |   | 1 |\nOnce the homography matrix H is calculated, we can use it to warp the source image onto the destination image, or vice versa, to align or overlay them correctly.\nApplications of projective transform in image processing include:\n\nImage stitching: To combine multiple images taken from different perspectives to create a panoramic image.\nAugmented reality: To superimpose virtual objects onto real-world scenes by aligning them correctly.\nImage rectification: To remove perspective distortion, making objects appear in a plane-parallel view.\nCamera calibration: To estimate camera parameters and correct image distortions in computer vision tasks.\n\nIt’s important to note that the projective transform assumes a pinhole camera model, which is a simplified model of real-world cameras that neglects certain optical distortions. For more accurate transformations, more complex camera models can be used, such as the radial distortion model in camera calibration.\n\n\nCode\nfrom skimage import transform\nfrom skimage.transform import resize\n\nh, w, _ = cropped_image.shape\n\nsrc = np.array([[0, 0], [0, w], [h, w], [h, 0]])\ndst = np.array([polygon_points[3], polygon_points[0], polygon_points[1], polygon_points[2]])\n\ntform3 = transform.ProjectiveTransform()\ntform3.estimate(src, dst)\nwarped = transform.warp(cropped_image, tform3, output_shape=(w, h))\n# flipped = np.flipud(warped)\n\nfinal = resize(warped, (w, w),\n                    anti_aliasing=True)\n\nplt.figure(figsize=(10,10))\nplt.imshow(final)\nplt.axis(\"off\")\n\n\n(-0.5, 2159.5, 2159.5, -0.5)\n\n\n\n\n\nLet’s break down what the core part of the code does:\n\nsrc = np.array([[0, 0], [0, w], [h, w], [h, 0]]): This line creates a NumPy array src containing four 2D points. These points represent the coordinates of a rectangular region in the source image that you want to transform. The points are given in (row, column) format, and they define a rectangular region with corners at (0,0), (0,w), (h,w), and (h,0). The w and h variables likely represent the width and height of the source image, respectively.\ndst = np.array([polygon_points[0], polygon_points[2], polygon_points[3], polygon_points[1]]): This line creates another NumPy array dst containing four 2D points. These points are the corresponding destination coordinates where you want the points from src to be mapped after the transformation. The polygon_points array likely contains four 2D points representing the desired corners of the transformed region.\ntform3 = transform.ProjectiveTransform(): This line creates an instance of the ProjectiveTransform class from the scikit-image library. This class represents a projective transformation (perspective transform) that can be used to transform points or images.\ntform3.estimate(src, dst): This line estimates the projective transformation based on the corresponding points defined in src and dst. It calculates the 3x3 homography matrix that maps the points in src to the points in dst, effectively determining the transformation required to warp the rectangular region defined by src to the desired shape represented by dst.\nwarped = transform.warp(cropped_image, tform3, output_shape=(w, h)): This line applies the estimated projective transformation to the cropped_image. The warp function from scikit-image is used for this purpose. The cropped_image is the source image from which you previously extracted the region defined by src. The output_shape=(w, h) argument specifies the dimensions of the output (warped) image, which should be the same as the dimensions of the original region defined by src. The tform3 transformation is applied to the cropped_image, resulting in the warped image.\n\nAfter this code executes, the warped image will be a transformed version of the original cropped_image, where the rectangular region defined by src is now warped to match the desired shape represented by the corresponding points in dst."
  },
  {
    "objectID": "utilities/segmenting_by_color.html",
    "href": "utilities/segmenting_by_color.html",
    "title": "Segmenting by colors in a image using LAB Space",
    "section": "",
    "text": "Segmenting by colors in a image\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom skimage.io import imread\nfrom skimage.color import rgb2lab, rgb2hsv\nfrom skimage.transform import rescale, resize\n\n# load the image\nurl = '../data/colors.jpg'\ncimage = imread(url)\ncimage = resize(cimage, (cimage.shape[0] // 4, cimage.shape[1] // 4),\n                       anti_aliasing=True)\n\n# convert the image from RGB to LAB\nlab_img = rgb2lab(cimage)\n\nfig, ax = plt.subplots(1, 2, figsize=(6,10))\nax[0].imshow(cimage)\nax[0].set_title('RGB color space')\n\nax[1].imshow(lab_img, cmap='gray')\nax[1].set_title('LAB color space')\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\nVisualizing the colors in image on the lab space\n\nfrom skimage import img_as_float\n# to plot the colors we will use the RGB values from the\n# image directly for colors. \nx,y, _ = cimage.shape\nto_plot = cimage.reshape(x * y, 3)\ncolors_map = img_as_float(to_plot)\n\n# create dataset for scatter plot\nscatter_x = []\nscatter_y = []\nfor xi in range(x):\n    for yi in range(y):\n        L_val = lab_img[xi,yi][0] \n        A_val = lab_img[xi,yi][1] \n        B_val = lab_img[xi,yi][2]\n        scatter_x.append(A_val)\n        scatter_y.append(B_val)\n        \nplt.figure(figsize=(8,8))\nplt.xlabel(\"a* from green to red\")\nplt.ylabel(\"b* from blue to yellow\")\nplt.scatter(scatter_x,scatter_y, c=colors_map)\nplt.show()\n\n\n\n\n\n\nFiltering the colors as per the above ranges in the plot\n\ndef filter_color(cimage, L_val_min, A_val_min, A_val_max, B_val_min, B_val_max, color):\n    filtered_image = np.copy(cimage)\n    for xi in range(x):\n        for yi in range(y):\n            L_val = lab_img[xi,yi][0] \n            A_val = lab_img[xi,yi][1] \n            B_val = lab_img[xi,yi][2]\n            if L_val &gt; L_val_min and A_val &gt; A_val_min and A_val &lt; A_val_max  and B_val &gt; B_val_min and B_val &lt; B_val_max:\n                filtered_image[xi, yi] = color\n            else:\n                filtered_image[xi, yi] = [255,255,255]   \n    return filtered_image\n\n\nred = filter_color(cimage, 30, 25, 100, 0, 100, [255, 0, 0])\n\n\nfig, ax = plt.subplots(1, 2, figsize=(6,10))\nax[0].imshow(cimage)\nax[0].set_title('RGB color space')\n\nax[1].imshow(red, cmap='gray')\nax[1].set_title('Red regions')\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\nblue = filter_color(cimage, 50,-40, 30, -128, -20, [0, 0, 255])\n\n\nfig, ax = plt.subplots(1, 2, figsize=(6,10))\nax[0].imshow(cimage)\nax[0].set_title('RGB color space')\n\nax[1].imshow(blue, cmap='gray')\nax[1].set_title('blue regions')\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\ngreen = filter_color(cimage, 50, -128, -20, 0, 50, [0, 255, 0])\n\n\nfig, ax = plt.subplots(1, 2, figsize=(6,10))\nax[0].imshow(cimage)\nax[0].set_title('RGB color space')\n\nax[1].imshow(green, cmap='gray')\nax[1].set_title('green regions')\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\nExpriment - Isolate Red, Blue and Green\n\nfiltered_image = np.copy(cimage)\nr = [30, 25, 100, 0, 100]\nb = [50,-40, 30, -128, -20]\ng = [50, -128, -20, 0, 50]\nfor xi in range(x):\n    for yi in range(y):\n        L_val = lab_img[xi,yi][0] \n        A_val = lab_img[xi,yi][1] \n        B_val = lab_img[xi,yi][2]\n        ## red\n        if L_val &gt; r[0] and A_val &gt; r[1] and A_val &lt; r[2]  and B_val &gt; r[3] and B_val &lt; r[4]:\n            filtered_image[xi, yi] = [255, 0, 0] # red\n        elif L_val &gt; b[0] and A_val &gt; b[1] and A_val &lt; b[2]  and B_val &gt; b[3] and B_val &lt; b[4]:\n            filtered_image[xi, yi] = [0, 0, 255] # blue\n        elif L_val &gt; g[0] and A_val &gt; g[1] and A_val &lt; g[2]  and B_val &gt; g[3] and B_val &lt; g[4]:\n            filtered_image[xi, yi] = [0, 255, 0] # green\n        else:\n            filtered_image[xi, yi] = [255,255,255]\n\nfig, ax = plt.subplots(1, 2, figsize=(6,10))\nax[0].imshow(cimage)\nax[0].set_title('RGB color space')\n\nax[1].imshow(filtered_image, cmap='gray')\nax[1].set_title('red, blue, green regions')\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers)."
  },
  {
    "objectID": "hand_detection.html",
    "href": "hand_detection.html",
    "title": "Hand Detection on Chess Board",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport cv2\n\n# load the image\nimage_path = 'data/frame_42_with_hand.jpg'\nimg = cv2.imread(image_path)\n#converting from gbr to hsv color space\nimg_HSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n#skin color range for hsv color space \nHSV_mask = cv2.inRange(img_HSV, (0, 15, 0), (17,170,255)) \nHSV_mask = cv2.morphologyEx(HSV_mask, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n\n#converting from gbr to YCbCr color space\nimg_YCrCb = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n#skin color range for hsv color space \nYCrCb_mask = cv2.inRange(img_YCrCb, (0, 135, 85), (255,180,135)) \nYCrCb_mask = cv2.morphologyEx(YCrCb_mask, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n\n#merge skin detection (YCbCr and hsv)\nglobal_mask=cv2.bitwise_and(YCrCb_mask,HSV_mask)\nglobal_mask=cv2.medianBlur(global_mask,3)\nglobal_mask = cv2.morphologyEx(global_mask, cv2.MORPH_OPEN, np.ones((4,4), np.uint8))\n\n\nHSV_result = cv2.bitwise_not(HSV_mask)\nYCrCb_result = cv2.bitwise_not(YCrCb_mask)\nglobal_result=cv2.bitwise_not(global_mask)\n\nplt.figure(figsize=(9,9))\nplt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\nplt.axis(\"off\")\nw,h=global_mask.shape\npercent_skin = (np.sum(global_mask != 0)/(w * h)) * 100\nif int(percent_skin) &gt; 1:\n    plt.text(100, 100, 'Hand Detected!', dict(size=30), c='r')\nplt.show()\n\n\n\n\nThe provided code performs skin color detection using two color spaces, HSV (Hue, Saturation, Value) and YCbCr (Luminance, Chrominance blue-difference, Chrominance red-difference). It combines the results from both color spaces to detect skin regions in an image.\nHere’s a step-by-step explanation of the code:\n\nConverting from BGR to HSV color space: The code starts by converting the input image img from the default BGR color space to the HSV color space using cv2.cvtColor(). HSV is a more suitable color space for skin detection due to its separation of color information into three components: Hue, Saturation, and Value.\nHSV skin color range: A binary mask is created using cv2.inRange() to filter out pixels within a specific HSV range that correspond to skin color. The lower and upper bounds for skin color in HSV are defined as (0, 15, 0) and (17, 170, 255), respectively. These values are set based on empirical observations and may need to be adjusted depending on the specific application and lighting conditions.\nMorphological opening on HSV mask: cv2.morphologyEx() is applied with the operation cv2.MORPH_OPEN to perform morphological opening on the HSV mask. Morphological opening is a combination of erosion followed by dilation and is used to remove small noise regions or small gaps in the detected skin areas. A small 3x3 square kernel is used for the operation.\nConverting from BGR to YCbCr color space: The code then converts the input image img from BGR to the YCbCr color space using cv2.cvtColor(). YCbCr separates the image into its luminance (Y) and chrominance (Cb and Cr) components.\nYCbCr skin color range: A binary mask is created using cv2.inRange() to filter out pixels within a specific YCbCr range that correspond to skin color. The lower and upper bounds for skin color in YCbCr are defined as (0, 135, 85) and (255, 180, 135), respectively. As with the HSV range, these values may require tuning based on specific conditions.\nMorphological opening on YCbCr mask: Similar to the HSV mask, a morphological opening operation is applied to the YCbCr mask using cv2.morphologyEx() with a 3x3 square kernel.\nMerging the skin detections from both color spaces: The two binary masks obtained from HSV and YCbCr are combined using a bitwise AND operation (cv2.bitwise_and()) to obtain the merged skin detection mask named global_mask. This step helps to include regions detected as skin in both color spaces.\nMedian blur on the merged mask: A median blur operation with a kernel size of 3 is applied to global_mask using cv2.medianBlur(). This helps to further smooth out the skin regions and remove small remaining noise.\nFinal morphological opening on the merged mask: A final morphological opening operation is performed on global_mask using a larger 4x4 square kernel to clean up any remaining small areas of non-skin pixels.\nGenerating the final results: The results for HSV, YCbCr, and the merged skin detection are obtained by inverting their respective masks using cv2.bitwise_not() to create binary images where skin pixels are represented by white and non-skin pixels by black. The results are stored in HSV_result, YCrCb_result, and global_result, respectively.\n\nAfter running this code, you should have three binary images: HSV_result, YCrCb_result, and global_result, where white pixels represent detected skin regions. The global_result should be the merged skin detection combining the information from both HSV and YCbCr color spaces."
  }
]