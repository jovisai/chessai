[
  {
    "objectID": "hello.html",
    "href": "hello.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "chess_board_segmentation.html",
    "href": "chess_board_segmentation.html",
    "title": "Chess Board Segmentation",
    "section": "",
    "text": "Applying Color Filtering to Image to isolate Red color\nTo segment the chess board from the environment, I am using a trick by coloring the boundaries of my chessboard as you can see in the below image.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport cv2\n\nimage = cv2.imread('data/chess_algo_1.jpg')\nimage_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# convert to LAB color space\nlab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n\n# Perform Otsu threshold on the A-channel \nth = cv2.threshold(lab[:,:,1], 127, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n\nresult = cv2.bitwise_and(image, image, mask=th)\n\n# Display the image using Matplotlib\nfig, ax = plt.subplots(nrows=1, ncols=2)\nax[0].set_title('Loaded Sample Image')\nax[0].imshow(image_rgb)\nax[0].axis('off')\nax[1].set_title('Red Color Segmented')\nax[1].imshow(result)\nax[1].axis('off')\n\nplt.tight_layout()\nplt.axis('off')\n# Display the plot\nplt.show()\n\n\n\n\n\nThe first step in the segmentation algorithm is to use the red boundary and isolate it from the full image.\nThis code snippet demonstrates how to perform color-based segmentation using the LAB color space and Otsu thresholding. Here’s a breakdown of the code:\n\nConvert to LAB Color Space:\n\nlab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB): Converts the original image from the default BGR color space to the LAB color space. The LAB color space consists of three channels: L (Lightness), A (green-magenta component), and B (blue-yellow component). This conversion is performed using the cvtColor function from OpenCV.\n\nPerform Otsu Thresholding on the A-channel:\n\nth = cv2.threshold(lab[:,:,1], 127, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]: Extracts the A-channel (green-magenta component) from the LAB image and applies Otsu’s thresholding technique to segment the image into foreground and background regions. Otsu’s thresholding automatically calculates the optimal threshold value based on the image histogram. The resulting binary threshold image is stored in the variable th.\n\nApply the Mask:\n\nimage = cv2.bitwise_and(image, image, mask=th): Applies the binary threshold mask to the original image using the bitwise_and function from OpenCV. This operation retains only the pixels in the original image that correspond to the foreground regions identified by the threshold mask. The mask argument specifies the binary mask to be applied.\n\n\nAfter executing this code, the image variable will hold the processed image, where only the foreground regions, determined by Otsu’s thresholding on the A-channel, are visible, and the background is set to black.\n\n\nIdentifying all the lines in the image using Classic straight-line Hough transform\nThe Hough transform is a simple algorithm commonly used in computer vision to detect lines and shapes in an image. It provides a robust method to identify geometric patterns by representing them in a parameter space known as the Hough space. The algorithm works by converting image space coordinates to parameter space, where each point in the parameter space corresponds to a possible line or shape in the image. By accumulating votes for different parameter combinations, the Hough transform identifies the most prominent lines or shapes based on the peaks in the parameter space. This approach is particularly useful for line detection, as it can handle various types of lines, including straight lines, curves, and even partially occluded or broken lines.\n\n\nCode\nfrom skimage.transform import hough_line, hough_line_peaks\nfrom skimage.color import rgb2gray\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimage_gray = rgb2gray(image_rgb)\n\n# Classic straight-line Hough transform\n# Set a precision of 0.5 degree.\ntested_angles = np.linspace(-np.pi / 2, np.pi / 2, 360, endpoint=False)\nh, theta, d = hough_line(rgb2gray(result), theta=tested_angles)\n\nplt.figure(figsize=(20,10))\nplt.imshow(image_gray, cmap='gray')\n\nfor _, angle, dist in zip(*hough_line_peaks(h, theta, d)):\n    (x0, y0) = dist * np.array([np.cos(angle), np.sin(angle)])\n    plt.axline((x0, y0), slope=np.tan(angle + np.pi/2))\n\nplt.title('Detected lines')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nHere is the explanation of the key part of code which does line detection using the Hough transform:\n\nGenerating tested angles\n\ntested_angles = np.linspace(-np.pi / 2, np.pi / 2, 360, endpoint=False)\nIn this line, the np.linspace() function generates an array of 360 equally spaced angles between -np.pi / 2 and np.pi / 2. These angles represent the range of lines to be tested during the Hough transform. The endpoint=False argument ensures that the endpoint is not included in the generated array.\n\nPerforming the Hough transform\n\nh, theta, d = hough_line(rgb2gray(result), theta=tested_angles)\nHere, the hough_line() function is applied to the grayscale version of the result image using rgb2gray() to convert it. The theta parameter is set to the tested_angles array, which specifies the angles to consider during the transform. The resulting Hough accumulator array h, along with the theta angles theta and distances d, are stored.\n\n\nCode\nplt.title(\"hough transform visualization\")\nplt.imshow(np.log(1 + h),\n           extent=[np.rad2deg(theta[-1]), np.rad2deg(theta[0]), d[-1], d[0]],\n           cmap='gray', aspect='auto')\n\n\n&lt;matplotlib.image.AxesImage at 0x7fcbb6fd7400&gt;\n\n\n\n\n\n\nDetecting and visualizing the lines\n\nfor _, angle, dist in zip(*hough_line_peaks(h, theta, d)):\n    (x0, y0) = dist * np.array([np.cos(angle), np.sin(angle)])\n    plt.axline((x0, y0), slope=np.tan(angle + np.pi/2))\nThis loop iterates through the peaks detected in the Hough transform using the hough_line_peaks() function. For each peak, the angle and dist values represent the orientation and distance from the origin to a line in the image. The (x0, y0) coordinates are calculated by multiplying the distance with the [np.cos(angle), np.sin(angle)] vector, which determines the line’s position. Finally, plt.axline() is used to draw a line on the plot using the calculated (x0, y0) coordinates and the tangent of the angle plus np.pi/2.\nBy running this code, you will perform the Hough transform to detect lines in an image. The resulting lines will be visualized on a plot using plt.axline(). This code segment is useful for line detection applications and helps to understand the concept of identifying lines in an image using the Hough transform.\n\n\nFind all points of intersections of the lines\nTo extract the end points of the chess board, we need to find the intersection of the Hough lines.\n\n\nCode\nimport math \n\ndef find_intersection_point(fp_x0, fp_y0, slope1, sp_x0, sp_y0, slope2):\n    if (slope1 - slope2) == 0:\n      return []\n\n    # Calculate the intersection point coordinates\n    x_intersect = (sp_y0 - fp_y0 + slope1 * fp_x0 - slope2 * sp_x0) / (slope1 - slope2)\n    y_intersect = slope1 * (x_intersect - fp_x0) + fp_y0\n\n    if x_intersect &lt; 0 or y_intersect &lt; 0 or x_intersect &gt; 4000 or y_intersect &gt; 4000:\n      return []\n\n    angle_of_intersection =  math.degrees(math.atan((slope1-slope2)/(1+slope1*slope2)))\n\n    if angle_of_intersection &lt; 45 and angle_of_intersection &gt; -45:\n      return []\n\n    # Intersection point coordinates\n    intersection_point = [x_intersect, y_intersect]\n\n    return intersection_point\n\nlines = []\nall_points_and_slopes = []\nfor _, angle, dist in zip(*hough_line_peaks(h, theta, d)):\n    (x0, y0) = dist * np.array([np.cos(angle), np.sin(angle)])\n    lines.append([x0, y0, angle])\n    # ax[2].axline((x0, y0), slope=np.tan(angle + np.pi/2))\n    # ax[2].scatter(x0, y0)\n    slope = np.tan(angle + np.pi/2)\n    all_points_and_slopes.append([x0, y0, slope])\n\n# find intersection points\nintersection_points = []\nfor i in range(len(all_points_and_slopes)):\n  for j in range(1, len(all_points_and_slopes)):\n    p1 = all_points_and_slopes[i]\n    p2 = all_points_and_slopes[j]\n\n    ip = find_intersection_point(p1[0], p1[1], p1[2], p2[0], p2[1], p2[2])\n    if ip:\n      intersection_points.append(ip)\n\nintersection_points = np.array(intersection_points)\n\nplt.figure(figsize=(20,10))\nplt.imshow(image_gray, cmap='gray')\nplt.scatter(intersection_points[:, 0], intersection_points[:, 1], color='r')\nplt.title('Detected points of intersections')\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nBut as you can see, there are simply too many points due to too many intersecting lines. We are mainly intereseted in just 4 points which represent the 4 corners of the chess board. We can use KMeans algorithm with a cluster size of 4 to group the close together points. The K-means clustering algorithm is commonly used for unsupervised learning tasks to group similar data points together. It is an iterative algorithm that aims to minimize the within-cluster variance by adjusting the cluster centroids until convergence.\n\n\nCode\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=4, random_state=0, n_init=\"auto\").fit(intersection_points)\n\nplt.figure(figsize=(20,10))\nplt.imshow(image_gray, cmap='gray')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], color='r')\nplt.title('Detected points of intersections')\n\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nkmeans = KMeans(n_clusters=4, random_state=0, n_init=\"auto\").fit(intersection_points)\nHere, the KMeans class is instantiated with the following parameters: - n_clusters=4: Specifies the number of clusters to form. In this case, we want to create four clusters. - random_state=0: Sets the random seed for reproducibility. By setting a specific random state value, the clustering results will be the same each time the code is run with the same data. - n_init=\"auto\": Determines the number of times the K-means algorithm will be run with different centroid seeds. “auto” automatically selects a value based on the number of data points.\nThe fit() method is then called on the KMeans object, with intersection_points as the input data. This fits the K-means model to the data, performing the clustering and assigning each data point to one of the four clusters.\nThe result of running the fit() method is stored in the kmeans variable. This object contains information about the fitted K-means model, including the cluster assignments for each data point.\nBy examining the kmeans object, you can access various properties and methods, such as kmeans.labels_ to retrieve the assigned cluster labels for each data point or kmeans.cluster_centers_ to obtain the centroid coordinates of each cluster.\n\n\nConnect the 4 points into a polygon\nWe use the standard convexHull algorithm to sort the 4 points in the order in which one can connect them into a polygon. Convex hull is a concept in computational geometry that represents the smallest convex polygon that encloses a given set of points in a plane.\n\n\nCode\nfrom scipy.spatial import ConvexHull\npoints = kmeans.cluster_centers_\nhull = ConvexHull(points)\n\nplt.figure(figsize=(20,10))\nplt.imshow(image_gray, cmap='gray')\nplt.plot(points[:,0], points[:,1], 'o', color='r')\nfor simplex in hull.simplices:\n    plt.plot(points[simplex, 0], points[simplex, 1], color='r')\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nRemove the rest of the environment\nIts a simple crop based on the polygon I generated from Convexhull calculation\n\n\nCode\nfrom skimage import draw\n\npoints = kmeans.cluster_centers_\npolygon_points = []\nfor k in hull.vertices:\n  polygon_points.append(points[k])\n\npolygon_points = np.array(polygon_points)\n\nprint(polygon_points)\n\n# Create a mask of the polygon region\nmask = np.zeros(image_rgb.shape[:2], dtype=np.uint8)\nrr, cc = draw.polygon(polygon_points[:, 1], polygon_points[:, 0])\nmask[rr, cc] = 1\n\n# Apply the mask to the input image\ncropped_image = image_rgb.copy()\ncropped_image[mask == 0] = 0\nplt.figure(figsize=(20,10))\nplt.imshow(cropped_image)\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n\n[[ 928.50000117 1361.24373308]\n [ 147.88027356 1358.97685454]\n [ 261.00292634  848.75466668]\n [ 827.86563763  843.80773074]]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Muthukrishnan",
    "section": "",
    "text": "Hey there, fellow adventurers! Welcome to my corner of this research notebook. I’m Muthukrishnan, your friendly neighborhood software engineer.\nI thrive on curiosity and a thirst for knowledge, constantly seeking new experiences and learning opportunities. Whether it’s diving into the depths of a gripping novel or immersing myself in the vibrant world of coding, I find joy in expanding my horizons.\nAs a tech enthusiast, I’m constantly amazed by the boundless possibilities it brings. From tinkering with gadgets to exploring the ever-evolving world of AI, I’m enthralled by the exciting intersection of humanity and innovation.\nIf you ask me my favorite miracles, it would be the Euler’s Identity and the Butterfly curve.\nEuler’s Identity \\[\\begin{gather*}\ne^{i\\pi }+1=0\n\\end{gather*}\\]\nButterfly curve \\[\\begin{gather*}\n{\\displaystyle x=\\sin t\\!\\left(e^{\\cos t}-2\\cos 4t-\\sin ^{5}\\!{\\Big (}{t \\over 12}{\\Big )}\\right)}\n\\end{gather*}\\]\n\\[\\begin{gather*}\n{\\displaystyle y=\\cos t\\!\\left(e^{\\cos t}-2\\cos 4t-\\sin ^{5}\\!{\\Big (}{t \\over 12}{\\Big )}\\right)}\n\\end{gather*}\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameterize the butterfly curve equation\nt = np.linspace(0, 2 * np.pi, 1000)\nx = np.sin(t) * (np.exp(np.cos(t)) - 2 * np.cos(4 * t) - np.sin(t / 12) ** 5)\ny = np.cos(t) * (np.exp(np.cos(t)) - 2 * np.cos(4 * t) - np.sin(t / 12) ** 5)\n\n# Plot the butterfly curve\nplt.plot(x, y, color='red', linewidth=1)\nplt.title(\"Butterfly curve\")\nplt.axis('equal')\n\n# Display the plot\nplt.show()\n\n\n\n\n\nSo, dear reader, join me on this thrilling journey as we navigate through the twists and turns of life. Together, let’s discover new passions, celebrate the extraordinary, and revel in the joy of being wonderfully unique.\nFasten your seatbelts, embrace your sense of wonder, and let’s embark on an unforgettable adventure together!\nI also blog at muthu.co, you can read about the Euler’s formula here"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computer Vision Notebooks",
    "section": "",
    "text": "Hey there, fellow adventurers! Welcome to my corner of this research notebook. I’m Muthukrishnan, your friendly neighborhood software engineer.\nI thrive on curiosity and a thirst for knowledge, constantly seeking new experiences and learning opportunities. Whether it’s diving into the depths of a gripping novel or immersing myself in the vibrant world of coding, I find joy in expanding my horizons.\nAs a tech enthusiast, I’m constantly amazed by the boundless possibilities it brings. From tinkering with gadgets to exploring the ever-evolving world of AI, I’m enthralled by the exciting intersection of humanity and innovation.\nIf you ask me my favorite miracles, it would be the Euler’s Identity and the Butterfly curve.\nEuler’s Identity \\[\\begin{gather*}\ne^{i\\pi }+1=0\n\\end{gather*}\\]\nButterfly curve \\[\\begin{gather*}\n{\\displaystyle x=\\sin t\\!\\left(e^{\\cos t}-2\\cos 4t-\\sin ^{5}\\!{\\Big (}{t \\over 12}{\\Big )}\\right)}\n\\end{gather*}\\]\n\\[\\begin{gather*}\n{\\displaystyle y=\\cos t\\!\\left(e^{\\cos t}-2\\cos 4t-\\sin ^{5}\\!{\\Big (}{t \\over 12}{\\Big )}\\right)}\n\\end{gather*}\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameterize the butterfly curve equation\nt = np.linspace(0, 2 * np.pi, 1000)\nx = np.sin(t) * (np.exp(np.cos(t)) - 2 * np.cos(4 * t) - np.sin(t / 12) ** 5)\ny = np.cos(t) * (np.exp(np.cos(t)) - 2 * np.cos(4 * t) - np.sin(t / 12) ** 5)\n\n# Plot the butterfly curve\nplt.plot(x, y, color='red', linewidth=1)\nplt.title(\"Butterfly curve\")\nplt.axis('equal')\n\n# Display the plot\nplt.show()\n\n\n\n\n\nSo, dear reader, join me on this thrilling journey as we navigate through the twists and turns of life. Together, let’s discover new passions, celebrate the extraordinary, and revel in the joy of being wonderfully unique.\nFasten your seatbelts, embrace your sense of wonder, and let’s embark on an unforgettable adventure together!\nI also blog at muthu.co, you can read about the Euler’s formula here"
  },
  {
    "objectID": "utilities/video to images.html",
    "href": "utilities/video to images.html",
    "title": "Extract Image frames from Video",
    "section": "",
    "text": "Code\nimport skimage.io\nimport os\n\n# Get the path to the video file.\nvideo_file = \"utilities/data/VOD_20230708_062617.mp4\"\n\nimport cv2\nimport os\n\n# Create a directory to store the images.\nimage_dir = \"video_frames\"\nif not os.path.exists(image_dir):\n    os.mkdir(image_dir)\n\n# Get the number of frames in the video.\ncap = cv2.VideoCapture(video_file)\nnum_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n# Iterate over the frames in the video and save them as images.\nfor i in range(num_frames):\n    ret, frame = cap.read()\n    if not ret:\n        continue\n    image_name = \"frame_{}.jpg\".format(i)\n    cv2.imwrite(os.path.join(image_dir, image_name), frame)\n\ncap.release()"
  },
  {
    "objectID": "utilities/video_to_images.html",
    "href": "utilities/video_to_images.html",
    "title": "Extract Image frames from Video",
    "section": "",
    "text": "import skimage.io\nimport os\n\n# Get the path to the video file.\nvideo_file = \"data/VOD_20230708_195627.mp4\"\n\nimport cv2\nimport os\n\n# Create a directory to store the images.\nimage_dir = \"video_frames\"\nif not os.path.exists(image_dir):\n    os.mkdir(image_dir)\n\n# Get the number of frames in the video.\ncap = cv2.VideoCapture(video_file)\nnum_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n# Iterate over the frames in the video and save them as images.\nfor i in range(num_frames):\n    ret, frame = cap.read()\n    if not ret:\n        continue\n    image_name = \"frame_{}.jpg\".format(i)\n    cv2.imwrite(os.path.join(image_dir, image_name), frame)\n\ncap.release()"
  },
  {
    "objectID": "chess_board_segmentation_with_flat_board.html",
    "href": "chess_board_segmentation_with_flat_board.html",
    "title": "Chess Board Segmentation with gray white board",
    "section": "",
    "text": "Applying Color Filtering to Image to isolate Red color\nTo segment the chess board from the environment, I am using a trick by coloring the boundaries of my chessboard as you can see in the below image.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport cv2\nimport urllib\nimport numpy as np\n\n# image = cv2.imread('http://192.168.1.20:8080/photo.jpg')\n\nurl = 'http://192.168.1.10:8080/photo.jpg'\nurl_response = urllib.request.urlopen(url)\nimg_array = np.array(bytearray(url_response.read()), dtype=np.uint8)\nimage = cv2.imdecode(img_array, -1)\n\nimage_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n# convert to LAB color space\nlab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n\n# Perform Otsu threshold on the A-channel \nth = cv2.threshold(lab[:,:,1], 127, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n\nresult = cv2.bitwise_and(image, image, mask=th)\n\n# Display the image using Matplotlib\nfig, ax = plt.subplots(nrows=1, ncols=2)\nax[0].set_title('Loaded Sample Image')\nax[0].imshow(image_rgb)\nax[0].axis('off')\nax[1].set_title('Red Color Segmented')\nax[1].imshow(result)\nax[1].axis('off')\n\nplt.tight_layout()\nplt.axis('off')\n# Display the plot\nplt.show()\n\n\n\n\n\nThe first step in the segmentation algorithm is to use the red boundary and isolate it from the full image.\nThis code snippet demonstrates how to perform color-based segmentation using the LAB color space and Otsu thresholding. Here’s a breakdown of the code:\n\nConvert to LAB Color Space:\n\nlab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB): Converts the original image from the default BGR color space to the LAB color space. The LAB color space consists of three channels: L (Lightness), A (green-magenta component), and B (blue-yellow component). This conversion is performed using the cvtColor function from OpenCV.\n\nPerform Otsu Thresholding on the A-channel:\n\nth = cv2.threshold(lab[:,:,1], 127, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]: Extracts the A-channel (green-magenta component) from the LAB image and applies Otsu’s thresholding technique to segment the image into foreground and background regions. Otsu’s thresholding automatically calculates the optimal threshold value based on the image histogram. The resulting binary threshold image is stored in the variable th.\n\nApply the Mask:\n\nimage = cv2.bitwise_and(image, image, mask=th): Applies the binary threshold mask to the original image using the bitwise_and function from OpenCV. This operation retains only the pixels in the original image that correspond to the foreground regions identified by the threshold mask. The mask argument specifies the binary mask to be applied.\n\n\nAfter executing this code, the image variable will hold the processed image, where only the foreground regions, determined by Otsu’s thresholding on the A-channel, are visible, and the background is set to black.\n\n\nIdentifying all the lines in the image using Classic straight-line Hough transform\nThe Hough transform is a simple algorithm commonly used in computer vision to detect lines and shapes in an image. It provides a robust method to identify geometric patterns by representing them in a parameter space known as the Hough space. The algorithm works by converting image space coordinates to parameter space, where each point in the parameter space corresponds to a possible line or shape in the image. By accumulating votes for different parameter combinations, the Hough transform identifies the most prominent lines or shapes based on the peaks in the parameter space. This approach is particularly useful for line detection, as it can handle various types of lines, including straight lines, curves, and even partially occluded or broken lines.\n\n\nCode\nfrom skimage.transform import hough_line, hough_line_peaks\nfrom skimage.color import rgb2gray\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimage_gray = rgb2gray(image_rgb)\n\n# Classic straight-line Hough transform\n# Set a precision of 0.5 degree.\ntested_angles = np.linspace(-np.pi / 2, np.pi / 2, 360, endpoint=False)\nh, theta, d = hough_line(rgb2gray(result), theta=tested_angles)\n\nplt.figure(figsize=(20,10))\nplt.imshow(image_gray, cmap='gray')\n\nfor _, angle, dist in zip(*hough_line_peaks(h, theta, d)):\n    (x0, y0) = dist * np.array([np.cos(angle), np.sin(angle)])\n    plt.axline((x0, y0), slope=np.tan(angle + np.pi/2))\n\nplt.title('Detected lines')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nHere is the explanation of the key part of code which does line detection using the Hough transform:\n\nGenerating tested angles\n\ntested_angles = np.linspace(-np.pi / 2, np.pi / 2, 360, endpoint=False)\nIn this line, the np.linspace() function generates an array of 360 equally spaced angles between -np.pi / 2 and np.pi / 2. These angles represent the range of lines to be tested during the Hough transform. The endpoint=False argument ensures that the endpoint is not included in the generated array.\n\nPerforming the Hough transform\n\nh, theta, d = hough_line(rgb2gray(result), theta=tested_angles)\nHere, the hough_line() function is applied to the grayscale version of the result image using rgb2gray() to convert it. The theta parameter is set to the tested_angles array, which specifies the angles to consider during the transform. The resulting Hough accumulator array h, along with the theta angles theta and distances d, are stored.\n\n\nCode\nplt.title(\"hough transform visualization\")\nplt.imshow(np.log(1 + h),\n           extent=[np.rad2deg(theta[-1]), np.rad2deg(theta[0]), d[-1], d[0]],\n           cmap='gray', aspect='auto')\n\n\n&lt;matplotlib.image.AxesImage at 0x7f1256d1a530&gt;\n\n\n\n\n\n\nDetecting and visualizing the lines\n\nfor _, angle, dist in zip(*hough_line_peaks(h, theta, d)):\n    (x0, y0) = dist * np.array([np.cos(angle), np.sin(angle)])\n    plt.axline((x0, y0), slope=np.tan(angle + np.pi/2))\nThis loop iterates through the peaks detected in the Hough transform using the hough_line_peaks() function. For each peak, the angle and dist values represent the orientation and distance from the origin to a line in the image. The (x0, y0) coordinates are calculated by multiplying the distance with the [np.cos(angle), np.sin(angle)] vector, which determines the line’s position. Finally, plt.axline() is used to draw a line on the plot using the calculated (x0, y0) coordinates and the tangent of the angle plus np.pi/2.\nBy running this code, you will perform the Hough transform to detect lines in an image. The resulting lines will be visualized on a plot using plt.axline(). This code segment is useful for line detection applications and helps to understand the concept of identifying lines in an image using the Hough transform.\n\n\nFind all points of intersections of the lines\nTo extract the end points of the chess board, we need to find the intersection of the Hough lines.\n\n\nCode\nimport math \n\ndef find_intersection_point(fp_x0, fp_y0, slope1, sp_x0, sp_y0, slope2):\n    if (slope1 - slope2) == 0:\n      return []\n\n    # Calculate the intersection point coordinates\n    x_intersect = (sp_y0 - fp_y0 + slope1 * fp_x0 - slope2 * sp_x0) / (slope1 - slope2)\n    y_intersect = slope1 * (x_intersect - fp_x0) + fp_y0\n\n    if x_intersect &lt; 0 or y_intersect &lt; 0 or x_intersect &gt; 4000 or y_intersect &gt; 4000:\n      return []\n\n    angle_of_intersection =  math.degrees(math.atan((slope1-slope2)/(1+slope1*slope2)))\n\n    if angle_of_intersection &lt; 45 and angle_of_intersection &gt; -45:\n      return []\n\n    # Intersection point coordinates\n    intersection_point = [x_intersect, y_intersect]\n\n    return intersection_point\n\nlines = []\nall_points_and_slopes = []\nfor _, angle, dist in zip(*hough_line_peaks(h, theta, d)):\n    (x0, y0) = dist * np.array([np.cos(angle), np.sin(angle)])\n    lines.append([x0, y0, angle])\n    # ax[2].axline((x0, y0), slope=np.tan(angle + np.pi/2))\n    # ax[2].scatter(x0, y0)\n    slope = np.tan(angle + np.pi/2)\n    all_points_and_slopes.append([x0, y0, slope])\n\n# find intersection points\nintersection_points = []\nfor i in range(len(all_points_and_slopes)):\n  for j in range(1, len(all_points_and_slopes)):\n    p1 = all_points_and_slopes[i]\n    p2 = all_points_and_slopes[j]\n\n    ip = find_intersection_point(p1[0], p1[1], p1[2], p2[0], p2[1], p2[2])\n    if ip:\n      intersection_points.append(ip)\n\nintersection_points = np.array(intersection_points)\n\nplt.figure(figsize=(20,10))\nplt.imshow(image_gray, cmap='gray')\nplt.scatter(intersection_points[:, 0], intersection_points[:, 1], color='r')\nplt.title('Detected points of intersections')\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n\n/tmp/ipykernel_182810/3893695547.py:14: RuntimeWarning: divide by zero encountered in double_scalars\n  angle_of_intersection =  math.degrees(math.atan((slope1-slope2)/(1+slope1*slope2)))\n\n\n\n\n\nBut as you can see, there are simply too many points due to too many intersecting lines. We are mainly intereseted in just 4 points which represent the 4 corners of the chess board. We can use KMeans algorithm with a cluster size of 4 to group the close together points. The K-means clustering algorithm is commonly used for unsupervised learning tasks to group similar data points together. It is an iterative algorithm that aims to minimize the within-cluster variance by adjusting the cluster centroids until convergence.\n\n\nCode\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=4, random_state=0, n_init=\"auto\").fit(intersection_points)\n\nplt.figure(figsize=(20,10))\nplt.imshow(image_gray, cmap='gray')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], color='r')\nplt.title('Detected points of intersections')\n\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nkmeans = KMeans(n_clusters=4, random_state=0, n_init=\"auto\").fit(intersection_points)\nHere, the KMeans class is instantiated with the following parameters: - n_clusters=4: Specifies the number of clusters to form. In this case, we want to create four clusters. - random_state=0: Sets the random seed for reproducibility. By setting a specific random state value, the clustering results will be the same each time the code is run with the same data. - n_init=\"auto\": Determines the number of times the K-means algorithm will be run with different centroid seeds. “auto” automatically selects a value based on the number of data points.\nThe fit() method is then called on the KMeans object, with intersection_points as the input data. This fits the K-means model to the data, performing the clustering and assigning each data point to one of the four clusters.\nThe result of running the fit() method is stored in the kmeans variable. This object contains information about the fitted K-means model, including the cluster assignments for each data point.\nBy examining the kmeans object, you can access various properties and methods, such as kmeans.labels_ to retrieve the assigned cluster labels for each data point or kmeans.cluster_centers_ to obtain the centroid coordinates of each cluster.\n\n\nConnect the 4 points into a polygon\nWe use the standard convexHull algorithm to sort the 4 points in the order in which one can connect them into a polygon. Convex hull is a concept in computational geometry that represents the smallest convex polygon that encloses a given set of points in a plane.\n\n\nCode\nfrom scipy.spatial import ConvexHull\npoints = kmeans.cluster_centers_\nhull = ConvexHull(points)\n\nplt.figure(figsize=(20,10))\nplt.imshow(image_gray, cmap='gray')\nplt.plot(points[:,0], points[:,1], 'o', color='r')\nfor simplex in hull.simplices:\n    plt.plot(points[simplex, 0], points[simplex, 1], color='r')\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nRemove the rest of the environment\nIts a simple crop based on the polygon I generated from Convexhull calculation\n\n\nCode\nfrom skimage import draw\n\npoints = kmeans.cluster_centers_\npolygon_points = []\nfor k in hull.vertices:\n  polygon_points.append(points[k])\n\npolygon_points = np.array(polygon_points)\n\n\n\npolygon_points[2][1] = polygon_points[2][1] - 50\npolygon_points[3][1] = polygon_points[2][1] - 50\n\nprint(polygon_points)\n\n# Create a mask of the polygon region\nmask = np.zeros(image_rgb.shape[:2], dtype=np.uint8)\nrr, cc = draw.polygon(polygon_points[:, 1], polygon_points[:, 0])\nmask[rr, cc] = 1\n\n# Apply the mask to the input image\ncropped_image = image_rgb.copy()\ncropped_image[mask == 0] = 0\nplt.figure(figsize=(20,10))\nplt.imshow(cropped_image)\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n\n[[ 896.86945895 2858.59575184]\n [1380.52322126 2131.49935418]\n [1692.93644219 2220.5996159 ]\n [1114.79920516 2170.5996159 ]]"
  },
  {
    "objectID": "utilities/segmenting_by_color.html",
    "href": "utilities/segmenting_by_color.html",
    "title": "Segmenting by colors in a image using LAB Space",
    "section": "",
    "text": "Segmenting by colors in a image\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom skimage.io import imread\nfrom skimage.color import rgb2lab, rgb2hsv\nfrom skimage.transform import rescale, resize\n\n# load the image\nurl = '../data/colors.jpg'\ncimage = imread(url)\ncimage = resize(cimage, (cimage.shape[0] // 4, cimage.shape[1] // 4),\n                       anti_aliasing=True)\n\n# convert the image from RGB to LAB\nlab_img = rgb2lab(cimage)\n\nfig, ax = plt.subplots(1, 2, figsize=(6,10))\nax[0].imshow(cimage)\nax[0].set_title('RGB color space')\n\nax[1].imshow(lab_img, cmap='gray')\nax[1].set_title('LAB color space')\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\nVisualizing the colors in image on the lab space\n\nfrom skimage import img_as_float\n# to plot the colors we will use the RGB values from the\n# image directly for colors. \nx,y, _ = cimage.shape\nto_plot = cimage.reshape(x * y, 3)\ncolors_map = img_as_float(to_plot)\n\n# create dataset for scatter plot\nscatter_x = []\nscatter_y = []\nfor xi in range(x):\n    for yi in range(y):\n        L_val = lab_img[xi,yi][0] \n        A_val = lab_img[xi,yi][1] \n        B_val = lab_img[xi,yi][2]\n        scatter_x.append(A_val)\n        scatter_y.append(B_val)\n        \nplt.figure(figsize=(8,8))\nplt.xlabel(\"a* from green to red\")\nplt.ylabel(\"b* from blue to yellow\")\nplt.scatter(scatter_x,scatter_y, c=colors_map)\nplt.show()\n\n\n\n\n\n\nFiltering the colors as per the above ranges in the plot\n\ndef filter_color(cimage, L_val_min, A_val_min, A_val_max, B_val_min, B_val_max, color):\n    filtered_image = np.copy(cimage)\n    for xi in range(x):\n        for yi in range(y):\n            L_val = lab_img[xi,yi][0] \n            A_val = lab_img[xi,yi][1] \n            B_val = lab_img[xi,yi][2]\n            if L_val &gt; L_val_min and A_val &gt; A_val_min and A_val &lt; A_val_max  and B_val &gt; B_val_min and B_val &lt; B_val_max:\n                filtered_image[xi, yi] = color\n            else:\n                filtered_image[xi, yi] = [255,255,255]   \n    return filtered_image\n\n\nred = filter_color(cimage, 30, 25, 100, 0, 100, [255, 0, 0])\n\n\nfig, ax = plt.subplots(1, 2, figsize=(6,10))\nax[0].imshow(cimage)\nax[0].set_title('RGB color space')\n\nax[1].imshow(red, cmap='gray')\nax[1].set_title('Red regions')\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\nblue = filter_color(cimage, 50,-40, 30, -128, -20, [0, 0, 255])\n\n\nfig, ax = plt.subplots(1, 2, figsize=(6,10))\nax[0].imshow(cimage)\nax[0].set_title('RGB color space')\n\nax[1].imshow(blue, cmap='gray')\nax[1].set_title('blue regions')\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\ngreen = filter_color(cimage, 50, -128, -20, 0, 50, [0, 255, 0])\n\n\nfig, ax = plt.subplots(1, 2, figsize=(6,10))\nax[0].imshow(cimage)\nax[0].set_title('RGB color space')\n\nax[1].imshow(green, cmap='gray')\nax[1].set_title('green regions')\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\nExpriment - Isolate Red, Blue and Green\n\nfiltered_image = np.copy(cimage)\nr = [30, 25, 100, 0, 100]\nb = [50,-40, 30, -128, -20]\ng = [50, -128, -20, 0, 50]\nfor xi in range(x):\n    for yi in range(y):\n        L_val = lab_img[xi,yi][0] \n        A_val = lab_img[xi,yi][1] \n        B_val = lab_img[xi,yi][2]\n        ## red\n        if L_val &gt; r[0] and A_val &gt; r[1] and A_val &lt; r[2]  and B_val &gt; r[3] and B_val &lt; r[4]:\n            filtered_image[xi, yi] = [255, 0, 0] # red\n        elif L_val &gt; b[0] and A_val &gt; b[1] and A_val &lt; b[2]  and B_val &gt; b[3] and B_val &lt; b[4]:\n            filtered_image[xi, yi] = [0, 0, 255] # blue\n        elif L_val &gt; g[0] and A_val &gt; g[1] and A_val &lt; g[2]  and B_val &gt; g[3] and B_val &lt; g[4]:\n            filtered_image[xi, yi] = [0, 255, 0] # green\n        else:\n            filtered_image[xi, yi] = [255,255,255]\n\nfig, ax = plt.subplots(1, 2, figsize=(6,10))\nax[0].imshow(cimage)\nax[0].set_title('RGB color space')\n\nax[1].imshow(filtered_image, cmap='gray')\nax[1].set_title('red, blue, green regions')\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers)."
  }
]